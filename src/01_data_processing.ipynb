{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating a single file from the transcripts. Creating a meta file.",
   "id": "4d9a25bdfdf01f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:55:39.386579Z",
     "start_time": "2025-04-08T10:55:39.044602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import requests"
   ],
   "id": "e983c926358549e7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = \"../data/combined_transcripts_freeConv\"\n",
    "\n",
    "docs = []\n",
    "meta = {}\n",
    "i = 0\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), 'r', encoding=\"UTF-8\") as f:\n",
    "        txt = f.read()\n",
    "\n",
    "    mordor = [e.strip() for e in txt.split(\"\\n\") if e.startswith(\"Mordor: \")]\n",
    "    gondor = [e.strip() for e in txt.split(\"\\n\") if e.startswith(\"Gondor: \")]\n",
    "\n",
    "    j = len(mordor)\n",
    "    k = len(gondor)\n",
    "    for h in range(j):\n",
    "        meta[i+h] = filename + \"_mordor_\" + str(h)\n",
    "    for g in range(k):\n",
    "        meta[i+g+j] = filename + \"_gondor_\" + str(g)\n",
    "    mordor = [e.replace(\"Mordor: \", \"\") for e in mordor]\n",
    "    gondor = [e.replace(\"Gondor: \", \"\") for e in gondor]\n",
    "    docs += mordor + gondor\n",
    "    a = (j+k)\n",
    "    i += a\n",
    "with open(\"../resources/docs.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(docs, outfile)\n",
    "\n",
    "with open(\"../resources/meta.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(meta, outfile)\n",
    "\n",
    "print(\"Az adatbázis\",len(docs), \"beszédfordulót tartalmaz.\")\n",
    "print(\"Az adatbázis első beszédfordulójának metaadata:\", meta[0])"
   ],
   "id": "22e75395cf3bbcf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# make txt file from pickle file",
   "id": "bd2b14ea0045d9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"../resources/docs.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)\n",
    "with open(\"../resources/conversation.txt\", \"w\", encoding='UTF-8') as f:\n",
    "    for doc in docs:\n",
    "        f.write(doc + \"\\n\")"
   ],
   "id": "9b3e5ef3ca496ec2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# separate the speech turns with \"csiribiri\" and make it lower case. Save it as txt file.",
   "id": "73fd5432690b1cdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"../resources/conversation.txt\", \"r\", encoding='UTF-8') as f:\n",
    "    conversation = f.read()\n",
    "conversation_2 = conversation.split(\"\\n\")\n",
    "conversation_3 = conversation_2[:-1]\n",
    "conversation_4 = \"\\ncsiribir\\n\".join(conversation_3)\n",
    "conversation_4 = conversation_4.lower()\n",
    "with open(\"../resources/conversation_csiribiri_lower.txt\", \"w\", encoding='UTF-8') as f:\n",
    "    f.write(conversation_4)"
   ],
   "id": "6ce3738ee85cccce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lemmatization and POS taging",
   "id": "268e6d94f6a70869"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#docker run --rm -p5000:5000 -it -e \"EMTSV_NUM_PROCESSES=4\" mtaril/emtsv",
   "id": "d6171432899334f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make the POST request\n",
    "text = requests.post('http://127.0.0.1:5000/tok/morph/pos',\n",
    "                  files={'file': open(\"../resources/conversation_csiribiri_lower.txt\", encoding='UTF-8')})\n",
    "# save file\n",
    "with open(\"../results/conversation_lower_emtsv.txt\", \"w\", encoding='UTF-8') as f:\n",
    "    f.write(text.text)"
   ],
   "id": "cbc79da9f2f87caf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:55:47.190590Z",
     "start_time": "2025-04-08T10:55:45.314021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# open txt file and make csv file from it\n",
    "with open(\"../results/conversation_lower_emtsv.txt\", \"r\", encoding='UTF-8') as f:\n",
    "    conversation = f.read()\n",
    "conversation_2 = conversation.split(\"\\n\")\n",
    "df = pd.DataFrame([x.split(\"\\t\") for x in conversation_2], columns=[\"form\", \"wsafter\", \"anas\", \"lemma\", \"xpostag\"])\n",
    "df_2 = df.copy().drop([\"wsafter\",\"anas\"], axis=1)"
   ],
   "id": "173fb236229ccad6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T19:49:01.280896Z",
     "start_time": "2025-04-04T19:49:00.626042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lementjük csv-ben a 3 oszlopot (form, lemma, POS-tag) tartalmazó dataframe-et \n",
    "df_2.to_csv(\"../results/conversation_lower_emtsv.csv\", index=False, encoding='UTF-8')"
   ],
   "id": "c00a5667899aec90",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T19:49:17.927168Z",
     "start_time": "2025-04-04T19:49:17.795519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# készítünk egy nyers lemmatizált változatot, amely tartalmazza az írásjeleket, a hümmögést, hezitálást, nevetést\n",
    "corpus_emtsv = pd.read_csv(\"../results/conversation_lower_emtsv.csv\", encoding='UTF-8')\n",
    "corpus_emtsv_2 = corpus_emtsv.dropna()\n",
    "lemmatized_raw_1 = corpus_emtsv_2[\"lemma\"].tolist()\n",
    "lemmatized_raw_1.pop(0)\n",
    "lemmatized_raw_2 = \" \".join(lemmatized_raw_1)\n",
    "lemmatized_raw_3 = lemmatized_raw_2.split(\"csiribir\")\n",
    "lemmatized_raw_4 = [x.split() for x in lemmatized_raw_3]"
   ],
   "id": "51056e78769fef68",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T19:49:23.423132Z",
     "start_time": "2025-04-04T19:49:23.244395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lementjük a lemmatizált szöveget txt-ben és pkl-ben, amely tartalmazza az írásjeleket, a hümmögést, hezitálást, nevetést\n",
    "with open(\"../results/lemmatized_raw.txt\", \"w\") as outfile:\n",
    "    for doc in lemmatized_raw_4:\n",
    "        outfile.write(\" \".join(doc) + \"\\n\")\n",
    "with open (\"../results/lemmatized_raw.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lemmatized_raw_4, f)"
   ],
   "id": "5f70ce9c474cd2b7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T19:49:31.471177Z",
     "start_time": "2025-04-04T19:49:31.074783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# készítünk egy lemmatizált változatot, amely nem tartalmazza az írásjeleket, a hümmögést, hezitálást, nevetést\n",
    "corpus_no_punctuation = corpus_emtsv_2[corpus_emtsv_2[\"xpostag\"] != \"[Punct]\"]\n",
    "corpus_no_hum = corpus_no_punctuation[corpus_no_punctuation[\"lemma\"] != \"hum\"]\n",
    "corpus_no_hes = corpus_no_hum[corpus_no_hum[\"lemma\"] != \"hes\"]\n",
    "corpus_no_laugh = corpus_no_hes[corpus_no_hes[\"lemma\"] != \"Laugh\"]\n",
    "corpus_no_laugh_2 = corpus_no_laugh.dropna()\n",
    "lemmatized_clean = corpus_no_laugh_2[\"lemma\"].tolist()\n",
    "lemmatized_clean.pop(0)\n",
    "lemmatized_clean_2 = \" \".join(lemmatized_clean)\n",
    "lemmatized_clean_3 = lemmatized_clean_2.split(\"csiribir\")\n",
    "lemmatized_clean_4 = [x.split() for x in lemmatized_clean_3]"
   ],
   "id": "b515f18cd18a0891",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T19:49:44.928477Z",
     "start_time": "2025-04-04T19:49:44.737643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lementjük a tiszta lemmatizált szöveget txt-ben és pkl-ben, amely nem tartalmazza az írásjeleket, a hümmögést, hezitálást, nevetést\n",
    "with open(\"../results/lemmatized_clean.txt\", \"w\") as outfile:\n",
    "    for doc in lemmatized_clean_4:\n",
    "        outfile.write(\" \".join(doc) + \"\\n\")\n",
    "with open (\"../results/lemmatized_clean.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lemmatized_clean_4, f)"
   ],
   "id": "a856f16593ae7ff6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T20:10:26.246938Z",
     "start_time": "2025-04-04T20:10:25.628071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# kiszedjük a POS-tageket csv-ben és pkl-ben, amely nem tartalmazza az írásjeleket, a hümmögést, hezitálást, nevetést. A POS-tagelő nem tudja, hogy a \"fog\" és a \"lesz\" jövő idő, ezért átírjuk őket.\n",
    "pos_table = corpus_no_laugh.copy()\n",
    "pos_table.loc[(pos_table[\"lemma\"] == \"lesz\") & (pos_table[\"xpostag\"].str.contains(\"Prs\")), \"xpostag\"] = pos_table[\"xpostag\"].str.replace(\"Prs\", \"Fut\")\n",
    "pos_table.loc[(pos_table[\"lemma\"] == \"fog\") & (pos_table[\"xpostag\"].str.contains(\"Prs\")), \"xpostag\"] = pos_table[\"xpostag\"].str.replace(\"Prs\", \"Fut\")\n",
    "pos_table = pos_table.drop(pos_table.index[0])"
   ],
   "id": "2bbe99790e1f31e1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T20:11:34.406588Z",
     "start_time": "2025-04-04T20:11:34.320533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lementjük csak a POS tageket pickle-ben és csv-ben, a teljesen tiszta változatot, amelyben nincsen írásjel, hümmögés, hezitálás, nevetés\n",
    "pos_table.to_csv(\"../results/postags_clean.csv\", index=False, encoding='UTF-8')\n",
    "pos_table.loc[pos_table[\"form\"] == \"csiribir\", \"xpostag\"] = \"csiribir\"\n",
    "pos_clean_1 = pos_table[\"xpostag\"].tolist()\n",
    "pos_clean_2 = \" \".join(pos_clean_1)\n",
    "pos_clean_3 = pos_clean_2.split(\"csiribir\")\n",
    "pos_clean_4 = [x.split() for x in pos_clean_3]\n",
    "with open (\"../results/postags_clean.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pos_clean_4, f)"
   ],
   "id": "7d055018f8f5a40d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Mondat tokenizálás",
   "id": "e38a73d07ae191e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import PunktTokenizer\n",
    "\n",
    "with open(\"../data/conversation.txt\", \"r\", encoding='UTF-8') as f:\n",
    "    conv_1 = f.read()\n",
    "conv_2 = conv_1.split(\"\\n\")\n",
    "conv_2.pop()\n",
    "tokenizer = PunktTokenizer()\n",
    "tokenized = [tokenizer.tokenize(line) for line in conv_2]\n",
    "pickle.dump(tokenized, open(\"../results/tokenized_by_sentence.pkl\", \"wb\"))"
   ],
   "id": "821efb4f6af227cb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
